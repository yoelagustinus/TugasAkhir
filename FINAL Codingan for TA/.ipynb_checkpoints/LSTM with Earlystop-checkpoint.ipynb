{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d14f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from datetime import date, timedelta, datetime \n",
    "from pandas.plotting import register_matplotlib_converters \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.dates as mdates \n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "import seaborn as sns \n",
    "import mysql.connector as mysql\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9f23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluasiForecasting:\n",
    "    def rmse_metric(actual, predicted):\n",
    "        mean_error = np.square(np.subtract(actual,predicted)).mean()\n",
    "        return math.sqrt(mean_error)\n",
    "\n",
    "    def mae_metric(actual, predicted):\n",
    "        y_true, predicted = np.array(actual), np.array(predicted)\n",
    "        return np.mean(np.abs(actual - predicted))\n",
    "\n",
    "    def mape_metric(actual, predicted): \n",
    "        actual, predicted = np.array(actual), np.array(predicted)\n",
    "        return np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "\n",
    "\n",
    "class DataLoad:\n",
    "    def read_data(start_date,end_date, symbol_dataset):\n",
    "        df = []\n",
    "        df = yf.download(symbol_dataset, start=start_date, end=end_date)\n",
    "        \n",
    "        return df\n",
    "\n",
    "class Preprocessing:\n",
    "    def feature_selection(df):\n",
    "        # Indexing Batches\n",
    "        train_df = df.sort_values(by=['Date']).copy()\n",
    "\n",
    "        # Daftar Fitur yang digunakan\n",
    "        FEATURES = ['High', 'Low', 'Open', 'Close', 'Volume']\n",
    "        data = pd.DataFrame(train_df)\n",
    "        data_filtered = data[FEATURES]\n",
    "\n",
    "        # menambahkan kolom prediksi dan menetapkan nilai dummy untuk menyiapkan data untuk penskalaan\n",
    "        data_filtered_ext = data_filtered.copy()\n",
    "        data_filtered_ext['Prediction'] = data_filtered_ext['Close']\n",
    "        return data_filtered_ext, data_filtered, data\n",
    "    \n",
    "    def reshape_data(data_filtered):\n",
    "        # Dapatkan jumlah baris dalam data\n",
    "        nrows = data_filtered.shape[0]\n",
    "\n",
    "        # Convert the data ke numpy values\n",
    "        np_data_unscaled = np.array(data_filtered)\n",
    "        np_data = np.reshape(np_data_unscaled, (nrows, -1))\n",
    "        \n",
    "        return np_data_unscaled, np_data\n",
    "\n",
    "    def min_max(np_data_unscaled,data_filtered_ext):\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        np_data_scaled = scaler.fit_transform(np_data_unscaled)\n",
    "\n",
    "        # Membuat scaler terpisah yang berfungsi pada satu kolom untuk prediksi penskalaan\n",
    "        scaler_pred = MinMaxScaler(feature_range=(0,1))\n",
    "        df_Close = pd.DataFrame(data_filtered_ext['Close'])\n",
    "        np_Close_scaled = scaler_pred.fit_transform(df_Close)\n",
    "        \n",
    "        return np_data_scaled, np_Close_scaled, scaler_pred\n",
    "    \n",
    "    def inverse_minmax(y_pred_scaled, y_test):\n",
    "        y_pred = scaler_pred.inverse_transform(y_pred_scaled)\n",
    "        y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))\n",
    "        \n",
    "        return y_pred, y_test_unscaled\n",
    "    \n",
    "    def partition_dataset(sequence_length, data, index_Close):\n",
    "        x, y = [], []\n",
    "        data_len = data.shape[0]\n",
    "        for i in range(sequence_length, data_len):\n",
    "            x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
    "            y.append(data[i, index_Close]) #contains the prediction values for validation,  for single-step prediction\n",
    "\n",
    "        # Convert the x and y to numpy arrays\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        return x, y\n",
    "    \n",
    "    def splitting_dataset(np_data_scaled,data):\n",
    "        # Set the sequence length - this is the timeframe used to make a single prediction\n",
    "        sequence_length = 1\n",
    "\n",
    "        # Prediction Index\n",
    "        index_Close = data.columns.get_loc(\"Close\")\n",
    "\n",
    "        # Split the training data into train and train data sets\n",
    "        # As a first step, we get the number of rows to train the model on 80% of the data \n",
    "        train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)\n",
    "\n",
    "        # Create the training and test data\n",
    "        train_data = np_data_scaled[0:train_data_len, :]\n",
    "        test_data = np_data_scaled[train_data_len - sequence_length:, :]\n",
    "        \n",
    "        # Generate training data and test data\n",
    "        x_train, y_train = Preprocessing.partition_dataset(sequence_length, train_data, index_Close)\n",
    "        x_test, y_test = Preprocessing.partition_dataset(sequence_length, test_data, index_Close)\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test, train_data_len\n",
    "class LSTM_unit:\n",
    "    def training_model(x_train, y_train, x_test, y_test, unit, epoch):\n",
    "        # Configure the neural network model\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(unit, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2]))))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss='mse')\n",
    "        # Training the model\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
    "\n",
    "        history = model.fit(x_train, y_train, \n",
    "                batch_size=16, \n",
    "                epochs=epoch,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks = early_stop)\n",
    "        return x_test, model, history\n",
    "        \n",
    "    def predict_model(x_test,model):\n",
    "        y_pred_scaled = model.predict(x_test)\n",
    "        return y_pred_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "arr_epochs  = [10]\n",
    "# 10  100  1000\n",
    "arr_units = [10]\n",
    "# 10  50   128\n",
    "start_date = \"2017-01-01\"\n",
    "\n",
    "arr_end_date = [ \"2017-03-31\" ]\n",
    "arr_symbol_dataset = [\"GGRM.jk\"]\n",
    "\n",
    "# arr_end_date = [\"2017-03-31\", \"2017-12-31\", \"2021-12-31\"]\n",
    "# arr_symbol_dataset = [\"GGRM.jk\", \"UNVR.jk\", \"PSDN.jk\"]\n",
    "\n",
    "jumlah_pengujian = 0\n",
    "for symbol_dataset in arr_symbol_dataset:\n",
    "    for end_date in arr_end_date:\n",
    "        for epoch in arr_epochs:\n",
    "            for unit in arr_units: \n",
    "                epochs = 0\n",
    "                jumlah_pengujian+=1\n",
    "\n",
    "                # load the time series Data use Yahoo Finance\n",
    "                df = DataLoad.read_data(start_date, end_date,symbol_dataset)\n",
    "\n",
    "                if df.shape[0]>=1250:\n",
    "                    term_status = \"long\"\n",
    "                elif df.shape[0]>=250:\n",
    "                    term_status = \"mid\"\n",
    "                else:\n",
    "                    term_status = \"short\"\n",
    "\n",
    "                #feature selection and scaling\n",
    "                data_filtered_ext, data_filtered, data = Preprocessing.feature_selection(df)\n",
    "                np_data_unscaled, np_data = Preprocessing.reshape_data(data_filtered)\n",
    "                np_data_scaled, np_Close_scaled, scaler_pred = Preprocessing.min_max(np_data_unscaled,data_filtered_ext)\n",
    "\n",
    "                #split train and test\n",
    "                x_train, y_train, x_test, y_test, train_data_len = Preprocessing.splitting_dataset(np_data_scaled,data) \n",
    "\n",
    "                # Train the Multivariable Prediction Model\n",
    "                x_test,model,history=LSTM_unit.training_model(x_train, y_train, x_test, y_test, unit, epoch)\n",
    "\n",
    "                # Predict data using data test\n",
    "                y_pred_scaled = LSTM_unit.predict_model(x_test,model)\n",
    "\n",
    "                #inverse minmax\n",
    "                y_pred, y_test_unscaled = Preprocessing.inverse_minmax(y_pred_scaled, y_test)\n",
    "\n",
    "                #Plot training & validation loss values\n",
    "                import matplotlib.pyplot as plt \n",
    "                def plot_metric(history, metric):\n",
    "                    train_metrics = history.history[metric]\n",
    "                    val_metrics = history.history['val_'+metric]\n",
    "                    epochs = range(1, len(train_metrics) + 1)\n",
    "                    plt.plot(epochs, train_metrics)\n",
    "                    plt.plot(epochs, val_metrics)\n",
    "                    plt.title('Training and validation '+ metric + ' ' + symbol_dataset +'_LSTM-'+ term_status + '_e='+ str(epoch) +'_u='+ str(unit))\n",
    "                    plt.xlabel(\"Epochs\")\n",
    "                    plt.ylabel(metric)\n",
    "                    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "                    plt.savefig(\"../results/LSTM_with_Earlystop/plots_metric/metric_loss_\" + symbol_dataset +'_LSTM-'+ term_status + '_e='+ str(epoch) +'_u='+ str(unit) + '.png')\n",
    "\n",
    "\n",
    "                plot_metric(history,'loss')\n",
    "\n",
    "                train_metrics = history.history['loss']\n",
    "                stop_epochs = len(train_metrics)\n",
    "\n",
    "\n",
    "                # Evaluate model performance\n",
    "                # Root Mean Square Error (RMSE)\n",
    "                RMSE = EvaluasiForecasting.rmse_metric(y_test_unscaled, y_pred)\n",
    "                RMSE = np.round(RMSE, 2)\n",
    "                print(f'Root Mean Square Error (RMSE): {RMSE}')\n",
    "\n",
    "                # Mean Absolute Error (MAE)\n",
    "                MAE = EvaluasiForecasting.mae_metric(y_test_unscaled, y_pred)\n",
    "                MAE = np.round(MAE, 2)\n",
    "                print(f'Median Absolute Error (MAE): {MAE}')\n",
    "\n",
    "                # Mean Absolute Percentage Error (MAPE)\n",
    "                MAPE = EvaluasiForecasting.mape_metric(y_test_unscaled, y_pred)\n",
    "                MAPE = np.round(MAPE, 2)\n",
    "                print(f'Mean Absolute Percentage Error (MAPE): {MAPE} %')\n",
    "\n",
    "                #save plot\n",
    "                # The date from which on the date is displayed\n",
    "                display_start_date = start_date\n",
    "\n",
    "                # Add the difference between the valid and predicted prices\n",
    "                train = pd.DataFrame(data_filtered_ext['Close'][:train_data_len + 1]).rename(columns={'Close': 'y_train'})\n",
    "                valid = pd.DataFrame(data_filtered_ext['Close'][train_data_len:]).rename(columns={'Close': 'y_test'})\n",
    "                valid.insert(1, \"y_pred\", y_pred, True)\n",
    "                # valid.insert(1, \"residuals\", valid[\"y_pred\"] - valid[\"y_test\"], True)\n",
    "                df_union = pd.concat([train, valid])\n",
    "\n",
    "                # Zoom in to a closer timeframe\n",
    "                df_union_zoom = df_union[df_union.index > display_start_date]\n",
    "\n",
    "                # Create the lineplot\n",
    "                # fig, ax1 = plt.subplots(figsize=(16, 8))\n",
    "                # plt.title(\"Predict Data vs Test Data\" + symbol_dataset +'_LSTM-'+ term_status + '_e='+ str(epoch) +'_u='+ str(unit))\n",
    "\n",
    "                # sns.set_palette([\"#FF0000\", \"#1960EF\", \"#00FF00\"])\n",
    "                # sns.lineplot(data=df_union_zoom[['y_pred', 'y_train', 'y_test']], linewidth=1.0, dashes=False, ax=ax1)\n",
    "                # plt.savefig(\"../results/LSTM_with_Earlystop/plots/plots_\" + symbol_dataset +'_LSTM-'+ term_status + '_e='+ str(epoch) +'_u='+ str(unit) + '.png')\n",
    "                # plt.legend()\n",
    "\n",
    "                #save to new dataset\n",
    "                new_data = pd.DataFrame(data_filtered_ext['Close'][train_data_len:]).rename(columns={'Close': 'real_close'})\n",
    "                new_data['close_lstm'] = y_pred\n",
    "                df_new_data = pd.DataFrame(new_data)\n",
    "                df_new_data.to_csv(\"../results/LSTM_with_Earlystop/datasets/\" + symbol_dataset +'_LSTM-'+ term_status + '_e='+ str(epoch) +'_u='+ str(unit) + '.csv', index=True)\n",
    "\n",
    "                obs_dataset = symbol_dataset+'-'+term_status\n",
    "\n",
    "                #connect database\n",
    "                mydb = mysql.connect(\n",
    "                    host=\"localhost\",\n",
    "                    user=\"root\",\n",
    "                    password=\"\",\n",
    "                    database=\"db_tugasakhir\"\n",
    "                )\n",
    "                mycursor = mydb.cursor()\n",
    "                \n",
    "\n",
    "                #insert to database\n",
    "                sql = \"INSERT INTO pengujian_lstm_with_earlystop (datasets, start_dates, end_dates,epochs, units, RMSE, MAE, MAPE, epoch_stop) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "                val = (obs_dataset, start_date, end_date, epoch, unit, RMSE, MAE, MAPE, stop_epochs)\n",
    "\n",
    "                mycursor.execute(sql,val)\n",
    "                mydb.commit()\n",
    "                print(\"pengujian ke: \" + str(jumlah_pengujian))\n",
    "                print(\"=================================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
