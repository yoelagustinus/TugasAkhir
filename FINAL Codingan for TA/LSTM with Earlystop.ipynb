{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f38c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from datetime import date, timedelta, datetime \n",
    "from pandas.plotting import register_matplotlib_converters \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.dates as mdates \n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "import seaborn as sns \n",
    "import mysql.connector as mysql\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01923f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluasiForecasting:\n",
    "    def rmse_metric(actual, predicted):\n",
    "        mean_error = np.square(np.subtract(actual,predicted)).mean()\n",
    "        return math.sqrt(mean_error)\n",
    "\n",
    "    def mae_metric(actual, predicted):\n",
    "        y_true, predicted = np.array(actual), np.array(predicted)\n",
    "        return np.mean(np.abs(actual - predicted))\n",
    "\n",
    "    def mape_metric(actual, predicted): \n",
    "        actual, predicted = np.array(actual), np.array(predicted)\n",
    "        return np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "\n",
    "\n",
    "class DataLoad:\n",
    "    def read_data(start_date,end_date, symbol_dataset):\n",
    "        df = []\n",
    "        df = yf.download(symbol_dataset, start=start_date, end=end_date)\n",
    "        \n",
    "        return df\n",
    "\n",
    "class Preprocessing:\n",
    "    def feature_selection(df):\n",
    "        # Indexing Batches\n",
    "        train_df = df.sort_values(by=['Date']).copy()\n",
    "\n",
    "        # Daftar Fitur yang digunakan\n",
    "        FEATURES = ['High', 'Low', 'Open', 'Close', 'Volume']\n",
    "        data = pd.DataFrame(train_df)\n",
    "        data_filtered = data[FEATURES]\n",
    "\n",
    "        # menambahkan kolom prediksi dan menetapkan nilai dummy untuk menyiapkan data untuk penskalaan\n",
    "        data_filtered_ext = data_filtered.copy()\n",
    "        data_filtered_ext['Prediction'] = data_filtered_ext['Close']\n",
    "        return data_filtered_ext, data_filtered, data\n",
    "    \n",
    "    def reshape_data(data_filtered):\n",
    "        # Dapatkan jumlah baris dalam data\n",
    "        nrows = data_filtered.shape[0]\n",
    "\n",
    "        # Convert the data ke numpy values\n",
    "        np_data_unscaled = np.array(data_filtered)\n",
    "        np_data = np.reshape(np_data_unscaled, (nrows, -1))\n",
    "        \n",
    "        return np_data_unscaled, np_data\n",
    "\n",
    "    def min_max(np_data_unscaled,data_filtered_ext):\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        np_data_scaled = scaler.fit_transform(np_data_unscaled)\n",
    "\n",
    "        # Membuat scaler terpisah yang berfungsi pada satu kolom untuk prediksi penskalaan\n",
    "        scaler_pred = MinMaxScaler(feature_range=(0,1))\n",
    "        df_Close = pd.DataFrame(data_filtered_ext['Close'])\n",
    "        np_Close_scaled = scaler_pred.fit_transform(df_Close)\n",
    "        \n",
    "        return np_data_scaled, np_Close_scaled, scaler_pred\n",
    "    \n",
    "    def inverse_minmax(y_pred_scaled, y_test):\n",
    "        y_pred = scaler_pred.inverse_transform(y_pred_scaled)\n",
    "        y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))\n",
    "        \n",
    "        return y_pred, y_test_unscaled\n",
    "    \n",
    "    def partition_dataset(sequence_length, data, index_Close):\n",
    "        x, y = [], []\n",
    "        data_len = data.shape[0]\n",
    "        for i in range(sequence_length, data_len):\n",
    "            x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
    "            y.append(data[i, index_Close]) #contains the prediction values for validation,  for single-step prediction\n",
    "\n",
    "        # Convert the x and y to numpy arrays\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        return x, y\n",
    "    \n",
    "    def splitting_dataset(np_data_scaled,data):\n",
    "        # Set the sequence length - this is the timeframe used to make a single prediction\n",
    "        sequence_length = 1\n",
    "\n",
    "        # Prediction Index\n",
    "        index_Close = data.columns.get_loc(\"Close\")\n",
    "\n",
    "        # Split the training data into train and train data sets\n",
    "        # As a first step, we get the number of rows to train the model on 80% of the data \n",
    "        train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)\n",
    "\n",
    "        # Create the training and test data\n",
    "        train_data = np_data_scaled[0:train_data_len, :]\n",
    "        test_data = np_data_scaled[train_data_len - sequence_length:, :]\n",
    "        \n",
    "        # Generate training data and test data\n",
    "        x_train, y_train = Preprocessing.partition_dataset(sequence_length, train_data, index_Close)\n",
    "        x_test, y_test = Preprocessing.partition_dataset(sequence_length, test_data, index_Close)\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test, train_data_len\n",
    "class LSTM_unit:\n",
    "    def training_model(x_train, y_train, x_test, y_test, unit, epoch):\n",
    "        # Configure the neural network model\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(unit, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2]))))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss='mse')\n",
    "        # Training the model\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
    "\n",
    "        history = model.fit(x_train, y_train, \n",
    "                batch_size=16, \n",
    "                epochs=epoch,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks = early_stop)\n",
    "        return x_test, model, history\n",
    "        \n",
    "    def predict_model(x_test,model):\n",
    "        y_pred_scaled = model.predict(x_test)\n",
    "        return y_pred_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad8de07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- GGRM.JK: No data found for this date range, symbol may be delisted\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (0,newaxis)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#feature selection and scaling\u001b[39;00m\n\u001b[0;32m     33\u001b[0m data_filtered_ext, data_filtered, data \u001b[38;5;241m=\u001b[39m Preprocessing\u001b[38;5;241m.\u001b[39mfeature_selection(df)\n\u001b[1;32m---> 34\u001b[0m np_data_unscaled, np_data \u001b[38;5;241m=\u001b[39m \u001b[43mPreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_filtered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m np_data_scaled, np_Close_scaled, scaler_pred \u001b[38;5;241m=\u001b[39m Preprocessing\u001b[38;5;241m.\u001b[39mmin_max(np_data_unscaled,data_filtered_ext)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#split train and test\u001b[39;00m\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mPreprocessing.reshape_data\u001b[1;34m(data_filtered)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Convert the data ke numpy values\u001b[39;00m\n\u001b[0;32m     42\u001b[0m np_data_unscaled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data_filtered)\n\u001b[1;32m---> 43\u001b[0m np_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp_data_unscaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np_data_unscaled, np_data\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (0,newaxis)"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "arr_epochs  = [10]\n",
    "# 10  100  1000\n",
    "arr_units = [10]\n",
    "# 10  50   128\n",
    "start_date = \"2017-01-01\"\n",
    "\n",
    "arr_end_date = [ \"2017-03-31\" ]\n",
    "arr_symbol_dataset = [\"GGRM.jk\"]\n",
    "\n",
    "# arr_end_date = [\"2017-03-31\", \"2017-12-31\", \"2021-12-31\"]\n",
    "# arr_symbol_dataset = [\"GGRM.jk\", \"UNVR.jk\", \"PSDN.jk\"]\n",
    "\n",
    "jumlah_pengujian = 0\n",
    "for symbol_dataset in arr_symbol_dataset:\n",
    "    for end_date in arr_end_date:\n",
    "        for epoch in arr_epochs:\n",
    "            for unit in arr_units: \n",
    "                epochs = 0\n",
    "                jumlah_pengujian+=1\n",
    "\n",
    "                # load the time series Data use Yahoo Finance\n",
    "                df = DataLoad.read_data(start_date, end_date,symbol_dataset)\n",
    "\n",
    "                if df.shape[0]>=1250:\n",
    "                    term_status = \"long\"\n",
    "                elif df.shape[0]>=250:\n",
    "                    term_status = \"mid\"\n",
    "                else:\n",
    "                    term_status = \"short\"\n",
    "\n",
    "                #feature selection and scaling\n",
    "                data_filtered_ext, data_filtered, data = Preprocessing.feature_selection(df)\n",
    "                np_data_unscaled, np_data = Preprocessing.reshape_data(data_filtered)\n",
    "                np_data_scaled, np_Close_scaled, scaler_pred = Preprocessing.min_max(np_data_unscaled,data_filtered_ext)\n",
    "\n",
    "                #split train and test\n",
    "                x_train, y_train, x_test, y_test, train_data_len = Preprocessing.splitting_dataset(np_data_scaled,data) \n",
    "\n",
    "                # Train the Multivariable Prediction Model\n",
    "                x_test,model,history=LSTM_unit.training_model(x_train, y_train, x_test, y_test, unit, epoch)\n",
    "\n",
    "                # Predict data using data test\n",
    "                y_pred_scaled = LSTM_unit.predict_model(x_test,model)\n",
    "\n",
    "                #inverse minmax\n",
    "                y_pred, y_test_unscaled = Preprocessing.inverse_minmax(y_pred_scaled, y_test)\n",
    "\n",
    "                #Plot training & validation loss values\n",
    "                import matplotlib.pyplot as plt \n",
    "                def plot_metric(history, metric):\n",
    "                    train_metrics = history.history[metric]\n",
    "                    val_metrics = history.history['val_'+metric]\n",
    "                    epochs = range(1, len(train_metrics) + 1)\n",
    "                    plt.plot(epochs, train_metrics)\n",
    "                    plt.plot(epochs, val_metrics)\n",
    "                    plt.title('Training and validation '+ metric + ' ' + symbol_dataset +'_LSTM-'+ term_status + '_e='+ str(epoch) +'_u='+ str(unit))\n",
    "                    plt.xlabel(\"Epochs\")\n",
    "                    plt.ylabel(metric)\n",
    "                    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "                    plt.savefig(\"../results/LSTM_with_Earlystop/plots_metric/metric_loss_\" + symbol_dataset +'_LSTM-'+ term_status + '_e='+ str(epoch) +'_u='+ str(unit) + '.png')\n",
    "\n",
    "\n",
    "                plot_metric(history,'loss')\n",
    "\n",
    "                train_metrics = history.history['loss']\n",
    "                stop_epochs = len(train_metrics)\n",
    "\n",
    "\n",
    "                # Evaluate model performance\n",
    "                # Root Mean Square Error (RMSE)\n",
    "                RMSE = EvaluasiForecasting.rmse_metric(y_test_unscaled, y_pred)\n",
    "                RMSE = np.round(RMSE, 2)\n",
    "                print(f'Root Mean Square Error (RMSE): {RMSE}')\n",
    "\n",
    "                # Mean Absolute Error (MAE)\n",
    "                MAE = EvaluasiForecasting.mae_metric(y_test_unscaled, y_pred)\n",
    "                MAE = np.round(MAE, 2)\n",
    "                print(f'Median Absolute Error (MAE): {MAE}')\n",
    "\n",
    "                # Mean Absolute Percentage Error (MAPE)\n",
    "                MAPE = EvaluasiForecasting.mape_metric(y_test_unscaled, y_pred)\n",
    "                MAPE = np.round(MAPE, 2)\n",
    "                print(f'Mean Absolute Percentage Error (MAPE): {MAPE} %')\n",
    "\n",
    "                #save plot\n",
    "                # The date from which on the date is displayed\n",
    "                display_start_date = start_date\n",
    "\n",
    "                # Add the difference between the valid and predicted prices\n",
    "                train = pd.DataFrame(data_filtered_ext['Close'][:train_data_len + 1]).rename(columns={'Close': 'y_train'})\n",
    "                valid = pd.DataFrame(data_filtered_ext['Close'][train_data_len:]).rename(columns={'Close': 'y_test'})\n",
    "                valid.insert(1, \"y_pred\", y_pred, True)\n",
    "                # valid.insert(1, \"residuals\", valid[\"y_pred\"] - valid[\"y_test\"], True)\n",
    "                df_union = pd.concat([train, valid])\n",
    "\n",
    "                # Zoom in to a closer timeframe\n",
    "                df_union_zoom = df_union[df_union.index > display_start_date]\n",
    "\n",
    "                # Create the lineplot\n",
    "                # fig, ax1 = plt.subplots(figsize=(16, 8))\n",
    "                # plt.title(\"Predict Data vs Test Data\" + symbol_dataset +'_LSTM-'+ term_status + '_e='+ str(epoch) +'_u='+ str(unit))\n",
    "\n",
    "                # sns.set_palette([\"#FF0000\", \"#1960EF\", \"#00FF00\"])\n",
    "                # sns.lineplot(data=df_union_zoom[['y_pred', 'y_train', 'y_test']], linewidth=1.0, dashes=False, ax=ax1)\n",
    "                # plt.savefig(\"../results/LSTM_with_Earlystop/plots/plots_\" + symbol_dataset +'_LSTM-'+ term_status + '_e='+ str(epoch) +'_u='+ str(unit) + '.png')\n",
    "                # plt.legend()\n",
    "\n",
    "                #save to new dataset\n",
    "                new_data = pd.DataFrame(data_filtered_ext['Close'][train_data_len:]).rename(columns={'Close': 'real_close'})\n",
    "                new_data['close_lstm'] = y_pred\n",
    "                df_new_data = pd.DataFrame(new_data)\n",
    "                df_new_data.to_csv(\"../results/LSTM_with_Earlystop/datasets/\" + symbol_dataset +'_LSTM-'+ term_status + '_e='+ str(epoch) +'_u='+ str(unit) + '.csv', index=True)\n",
    "\n",
    "                obs_dataset = symbol_dataset+'-'+term_status\n",
    "\n",
    "                #connect database\n",
    "                mydb = mysql.connect(\n",
    "                    host=\"localhost\",\n",
    "                    user=\"root\",\n",
    "                    password=\"\",\n",
    "                    database=\"db_tugasakhir\"\n",
    "                )\n",
    "                mycursor = mydb.cursor()\n",
    "                \n",
    "\n",
    "                #insert to database\n",
    "                sql = \"INSERT INTO pengujian_lstm_with_earlystop (datasets, start_dates, end_dates,epochs, units, RMSE, MAE, MAPE, epoch_stop) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "                val = (obs_dataset, start_date, end_date, epoch, unit, RMSE, MAE, MAPE, stop_epochs)\n",
    "\n",
    "                mycursor.execute(sql,val)\n",
    "                mydb.commit()\n",
    "                print(\"pengujian ke: \" + str(jumlah_pengujian))\n",
    "                print(\"=================================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
